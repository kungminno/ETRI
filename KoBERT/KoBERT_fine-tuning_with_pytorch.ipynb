{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pmBYohJjMF0U","I_4QOqLWNJVk","HsYQ2oD3Nb7t","5ksZKGMAOGzf","zS0MZDcTNHJm","3PYhtHoBWPMD"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Install KoBERT"],"metadata":{"id":"pmBYohJjMF0U"}},{"cell_type":"code","source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"id":"PHFKOjGCDgC2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Used Library\n","\n"],"metadata":{"id":"I_4QOqLWNJVk"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import csv\n","import os\n","import collections\n","import json\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from transformers import BertModel\n","\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import classification_report\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"HGbyvdV7NJfo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Class and function definitions"],"metadata":{"id":"HsYQ2oD3Nb7t"}},{"cell_type":"code","source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n"," \n","    def __len__(self):\n","        return (len(self.labels))"],"metadata":{"id":"7vxMWfQRyOe6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768, \n","                 num_classes=7,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        else:\n","            out = pooler\n","        return self.classifier(out)"],"metadata":{"id":"rOa9ZzftyRFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FocalLoss(nn.Module):\n","    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, input, target):\n","        BCE_loss = F.cross_entropy(input, target, reduction='none')\n","        pt = torch.exp(-BCE_loss)\n","        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n","\n","        if self.reduction == 'mean':\n","            return F_loss.mean()\n","        elif self.reduction == 'sum':\n","            return F_loss.sum()\n","        else:\n","            return F_loss"],"metadata":{"id":"qpSLM2g64hLd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"metadata":{"id":"iMkAjnvwyW5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_models(state_dict_filepath):\n","    model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n","    model.load_state_dict(torch.load(state_dict_filepath))\n","    model.eval()\n","\n","    loss_fn = FocalLoss()\n","\n","    test_losses = []\n","    test_accs = []\n","    test_acc = 0.0\n","    test_loss = 0.0\n","    with torch.no_grad():\n","        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","            token_ids = token_ids.long().to(device)\n","            segment_ids = segment_ids.long().to(device)\n","            valid_length = valid_length\n","            label = label.long().to(device)\n","            out = model(token_ids, valid_length, segment_ids)\n","            loss = loss_fn(out, label)\n","            test_acc += calc_accuracy(out, label)\n","            test_loss += loss.item()\n","        test_loss /= (batch_id+1)\n","        test_acc /= (batch_id+1)\n","        test_losses.append(test_loss)\n","        test_accs.append(test_acc)\n","\n","    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n","\n","    return test_acc"],"metadata":{"id":"IQn-5RzAyacV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def new_softmax(a):\n","    c = np.max(a)\n","    exp_a = np.exp(a - c)\n","    sum_exp_a = np.sum(exp_a)\n","    y = (exp_a / sum_exp_a) * 100\n","    return np.round(y, 3)"],"metadata":{"id":"G4HckNMdyiYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(predict_sentence, true_label):\n","    data = [predict_sentence, true_label]\n","    dataset_another = [data]\n","\n","    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n","\n","    model.eval()\n","\n","    y_true = []\n","    y_pred = []\n","    probabilities = []\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        label = label.long().to(device)\n","\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","        for i in out:\n","            logits = i\n","            logits = logits.detach().cpu().numpy()\n","\n","            logits = np.round(new_softmax(logits), 3).tolist()\n","            predicted_probabilities = [np.round(logit, 3) for logit in logits]\n","\n","            emotion_classes = [\"fear\", \"surprise\", \"angry\", \"sad\", \"neutral\", \"happy\", \"disgust\"]\n","            predicted_label = np.argmax(logits)\n","            predicted_emotion = emotion_classes[predicted_label]\n","\n","            y_true.append(int(true_label))\n","            y_pred.append(predicted_label)\n","            probabilities.append(predicted_probabilities)\n","\n","    return y_true, y_pred, probabilities\n"],"metadata":{"id":"FUu-2SZLKxyy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Learning"],"metadata":{"id":"5ksZKGMAOGzf"}},{"cell_type":"code","source":["bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","### Path setting is required.\n","dataset_train = pd.read_csv('train_text_dataset.csv', encoding='utf-8')\n","dataset_train.dropna(inplace=True)\n","dataset_train = dataset_train[dataset_train != '.'].dropna()\n","\n","dataset_train_list = [[str(text), int(label)] for text, label in zip(dataset_train['Text'], dataset_train['Lable'])]\n","\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 10  \n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate = 5e-5\n","\n","k_folds = 5\n","skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","for fold, (train_index, val_index) in enumerate(skf.split(dataset_train_list, [item[1] for item in dataset_train_list])):\n","    print(f\"Fold {fold + 1}\")\n","\n","    train_data = [dataset_train_list[i] for i in train_index]\n","    val_data = [dataset_train_list[i] for i in val_index]\n","    data_train = BERTDataset(train_data, 0, 1, tok, max_len, True, False)\n","    data_val = BERTDataset(val_data, 0, 1, tok, max_len, True, False)\n","    train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","    val_dataloader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, num_workers=5)\n","\n","    model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n","\n","    t_total = len(train_dataloader) * num_epochs\n","    warmup_step = int(t_total * warmup_ratio)\n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","    loss_fn = FocalLoss()\n","    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n","\n","    for e in range(num_epochs):\n","      train_acc, train_loss, val_acc, val_loss = 0.0, 0.0, 0.0, 0.0\n","      model.train()\n","\n","      for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","          optimizer.zero_grad()\n","          token_ids = token_ids.long().to(device)\n","          segment_ids = segment_ids.long().to(device)\n","          valid_length= valid_length\n","          label = label.long().to(device)\n","          out = model(token_ids, valid_length, segment_ids)\n","          loss = loss_fn(out, label)\n","          loss.backward()\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","          optimizer.step()\n","          scheduler.step()\n","          train_acc += calc_accuracy(out, label)\n","          train_loss += loss.item()\n","          if batch_id % log_interval == 0:\n","              print(f\"epoch {e+1} batch id {batch_id+1} loss {loss.data.cpu().numpy()} train acc {train_acc / (batch_id+1)}\")\n","      train_loss /= (batch_id+1)\n","      train_acc /= (batch_id+1)\n","      train_losses.append(train_loss)\n","      train_accs.append(train_acc)\n","      print(f\"epoch {e+1} train loss {train_loss} train acc {train_acc}\")\n","          \n","      model.eval()\n","      with torch.no_grad():\n","        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(val_dataloader):\n","            token_ids = token_ids.long().to(device)\n","            segment_ids = segment_ids.long().to(device)\n","            label = label.long().to(device)\n","            out = model(token_ids, valid_length, segment_ids)\n","            loss = loss_fn(out, label)\n","            val_acc += calc_accuracy(out, label)\n","            val_loss += loss.item()\n","        val_loss /= (batch_id + 1)\n","        val_acc /= (batch_id + 1)\n","        val_losses.append(val_loss)\n","        val_accs.append(val_acc)\n","\n","      print(f\"epoch {e+1} val loss {val_loss} val acc {val_acc}\")\n","\n","    ### Path setting is required.\n","    PATH = 'your/path'\n","    fold_path = PATH + f\"Focal_KoBERT_e10_b64_fold_{fold + 1}\"\n","    state_dict_path = fold_path + \"_state_dict.pt\"\n","    torch.save(model, fold_path + \".pt\")\n","    torch.save(model.state_dict(), state_dict_path)\n","    torch.save({\n","        \"model\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict()\n","    }, fold_path + \"_all.tar\")\n","\n","    state_dict_paths = []\n","    state_dict_paths.append(state_dict_path)\n","\n","\n","fig, axs = plt.subplots(figsize=(10, 10))\n","axs.plot(train_losses, label=\"Train loss\")\n","axs.plot(val_losses, label=\"Val loss\")\n","axs.plot(train_accs, label=\"Train acc\")\n","axs.plot(val_accs, label=\"Val acc\")\n","axs.set_title(\"Focal_e10_b64_f5\")\n","axs.legend()\n","plt.savefig(PATH + \"Focal_e10_b64_f5.png\")\n","plt.show()"],"metadata":{"id":"OufuEe-6DkGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Learned model Test"],"metadata":{"id":"zS0MZDcTNHJm"}},{"cell_type":"code","source":["### Path setting is required.\n","csv_file_path = 'test_text_dataset.csv'\n","dataset_test = pd.read_csv(csv_file_path, encoding='utf-8') \n","dataset_test.dropna(inplace=True)\n","dataset_test = dataset_test[dataset_test != '.'].dropna()\n","\n","dataset_test_list = [[str(text), int(label)] for text, label in zip(dataset_test['Text'], dataset_test['Lable'])]\n","\n","test_data = BERTDataset(dataset_test_list, 0, 1, tok, max_len, True, False)\n","test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=5)\n","\n","accuracies = []\n","for state_dict_filepath in state_dict_filepaths:\n","    test_accuracy = test_models(state_dict_filepath)\n","    accuracies.append(test_accuracy)\n","\n","best_model_index = accuracies.index(max(accuracies))\n","best_model_filepath = state_dict_filepaths[best_model_index]\n","print(\"Best model filepath:\", best_model_filepath)"],"metadata":{"id":"lIkF05KTeMXJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = torch.load(best_model_filepath.replace(\"_state_dict\", \"\")) \n","model.load_state_dict(torch.load(best_model_filepath))\n","\n","predictions = []\n","with torch.no_grad():\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","\n","        outputs = model(token_ids, valid_length, segment_ids)\n","        _, predicted = torch.max(outputs, dim=1)\n","        predictions.extend(predicted.tolist())\n","\n","target_names = [\"fear\", \"surprise\", \"angry\", \"sad\", \"neutral\", \"happy\", \"disgust\"]\n","print(classification_report(test_data.labels, predictions, target_names=target_names))"],"metadata":{"id":"LOQvDknkzT0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Learned model Prediction"],"metadata":{"id":"3PYhtHoBWPMD"}},{"cell_type":"code","source":["model = torch.load(best_model_filepath.replace(\"_state_dict\", \"\"))\n","model.load_state_dict(torch.load(best_model_filepath))\n","\n","### Path setting is required.\n","csv_file_path = 'test_text_dataset.csv'\n","\n","with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n","    csv_reader = csv.reader(csvfile)\n","    \n","    next(csv_reader)\n","\n","    y_true_list = []  \n","    y_pred_list = []  \n","\n","    for row in csv_reader:\n","        sentence = row[2]\n","        true_label = row[3]\n","\n","        y_true, y_pred, probabilities = predict(sentence, true_label)\n","\n","        target_names = [\"fear\", \"surprise\", \"angry\", \"sad\", \"neutral\", \"happy\", \"disgust\"]\n","        y_true_names = [target_names[i] for i in y_true]\n","        y_pred_names = [target_names[i] for i in y_pred]\n","\n","        y_true_list.append(y_true_names)\n","        y_pred_list.append(y_pred_names)\n","        \n","        print(f\"{row[0]}, {sentence}: True Label = {y_true}, Prediction = {y_pred}, Probability = \" + probabilities)"],"metadata":{"id":"NA69m8DnSKLr"},"execution_count":null,"outputs":[]}]}