{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc97139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa \n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import collections\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd659895",
   "metadata": {},
   "source": [
    "<span style='background-color:#fff5b1'>data_dir는 사용자에 맞게 변경 필요</span>\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87564c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/kungm/Desktop/ETRI/example/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181eb96",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "============="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0cbe0",
   "metadata": {},
   "source": [
    "<span style='background-color:#fff5b1'>file_path, wav_path, annotation_root는 사용자에 맞게 변경 필요</span>\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "모든 session.csv 파일 불러와서 id,emotion,label,wav_path 정보만 담아서 합치기\n",
    "train_set : test set = 8 :2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_extraction(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    col = df.columns\n",
    "\n",
    "    eval_col = [index for index in col if 'Eval' in index]\n",
    "\n",
    "    eval_df = df['Segment ID']\n",
    "    for index in eval_col:\n",
    "        _df = df[index]\n",
    "        eval_df = pd.concat([eval_df, _df], axis=1)\n",
    "\n",
    "    eval_df.drop([0], axis=0, inplace=True)\n",
    "    #19파일은 .csv파일 에 f/m가 들어가며 그거에 따라서 화자/청자가 정해짐. 그거 구분 위함\n",
    "    for i in eval_df.index:\n",
    "        if (filepath[-9:-8] != eval_df['Segment ID'][i][-4:-3]):\n",
    "            eval_df.drop(i, axis=0, inplace=True)\n",
    "        \n",
    "    eval_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    indices = np.arange(len(eval_df))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return eval_df, indices\n",
    "\n",
    "\n",
    "def process_emotion(dataframe, dictionary, idx_array):\n",
    "    for i in idx_array:\n",
    "        total_eval = dataframe.iloc[i, 1]\n",
    "        eval_sum = dataframe.iloc[i, 2:].to_list()\n",
    "        eval_dict = dict(collections.Counter(eval_sum))\n",
    "\n",
    "        for key, value in eval_dict.items():\n",
    "            if key in total_eval:\n",
    "                file_path = f\"C:/Users/kungm/Desktop/ETRI/KEMDy19/wav/Session{dataframe.iloc[i, 0][4:6]}/{dataframe.iloc[i, 0][:-5]}/{dataframe.iloc[i, 0]}.txt\"\n",
    "                wav_path = f\"C:/Users/kungm/Desktop/ETRI/KEMDy19/wav/Session{dataframe.iloc[i, 0][4:6]}/{dataframe.iloc[i, 0][:-5]}/{dataframe.iloc[i, 0]}.wav\"\n",
    "                if not os.path.exists(file_path):\n",
    "                    continue\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    file_content = file.read()\n",
    "                    file_content = file_content.replace('c/', '').replace('n/', '').replace('N/', '').replace('u/', '').replace('l/', '').replace('b/', '').replace('s/', '').replace('o/', '').replace('*', '').replace('+', '').replace('/', '').replace('(', '').replace(')', '')\n",
    "                if (value >= 6):\n",
    "                    if (key == 'fear'):\n",
    "                        dictionary[dataframe.iloc[i, 0]] = dict(Emotion=key, Text=file_content[:-1], Label=0, Path=wav_path)\n",
    "                    elif (key == 'surprise'):\n",
    "                        dictionary[dataframe.iloc[i, 0]] = dict(Emotion=key, Text=file_content[:-1], Label=1, Path=wav_path)\n",
    "                    elif (key == 'angry'):\n",
    "                        dictionary[dataframe.iloc[i, 0]] = dict(Emotion=key, Text=file_content[:-1], Label=2, Path=wav_path)\n",
    "                    elif (key == 'sad'):\n",
    "                        dictionary[dataframe.iloc[i, 0]] = dict(Emotion=key, Text=file_content[:-1], Label=3, Path=wav_path)\n",
    "                    elif (key == 'neutral'):\n",
    "                        dictionary[dataframe.iloc[i, 0]] = dict(Emotion=key, Text=file_content[:-1], Label=4, Path=wav_path)\n",
    "                    elif (key == 'happy'):\n",
    "                        dictionary[dataframe.iloc[i, 0]] = dict(Emotion=key, Text=file_content[:-1], Label=5, Path=wav_path)\n",
    "                    elif (key == 'disgust'):\n",
    "                        dictionary[dataframe.iloc[i, 0]] = dict(Emotion=key, Text=file_content[:-1], Label=6, Path=wav_path)\n",
    "\n",
    "\n",
    "def save_json(dictionary, file_name):\n",
    "    json_data = json.dumps(dictionary, ensure_ascii=False)\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(json_data)\n",
    "        \n",
    "def json_to_csv(json_file, csv_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write header\n",
    "        writer.writerow(['ID', 'Emotion', 'Text', 'Label', 'Path'])\n",
    "        \n",
    "        # Write data\n",
    "        for key, value in data.items():\n",
    "            row = [key, value['Emotion'], value['Text'], value['Label'], value['Path']]\n",
    "            writer.writerow(row)\n",
    "        \n",
    "\n",
    "def main():\n",
    "    train_final = dict()\n",
    "    test_final = dict()\n",
    "\n",
    "\n",
    "    annotation_root = \"C:/Users/kungm/Desktop/ETRI/KEMDy19/annotation\"\n",
    "    csv_files = os.listdir(annotation_root)\n",
    "\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = annotation_root + '/' + csv_file\n",
    "        dataframe, idx_array = column_extraction(csv_path)\n",
    "        \n",
    "        train_indices = idx_array[:int(len(dataframe) * 0.8)]\n",
    "        test_indices = idx_array[int(len(dataframe) * 0.8):]\n",
    "        \n",
    "        process_emotion(dataframe, train_final, train_indices)\n",
    "        process_emotion(dataframe, test_final, test_indices)\n",
    "\n",
    "    save_json(train_final, data_dir + 'train.json')\n",
    "    save_json(test_final, data_dir + 'test.json')\n",
    "    \n",
    "    json_to_csv(data_dir + 'train.json', data_dir + 'train.csv')\n",
    "    json_to_csv(data_dir + 'test.json', data_dir + 'val.csv')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54a554",
   "metadata": {},
   "source": [
    ".json 파일 tranpose 후 다시 저장\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "\n",
    "with open(data_dir+\"train.json\", 'r',encoding='UTF8') as file:\n",
    "    train = json.load(file)\n",
    "    \n",
    "with open(data_dir+\"test.json\", 'r',encoding='UTF8') as file:\n",
    "    test = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dfb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#오디오 전처리에 사용하기 편하게 파일정리\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "train_df=train_df.transpose()\n",
    "train_df=train_df.reset_index()\n",
    "train_df=train_df.rename(columns={'index':'ID'})\n",
    "train_df=train_df.drop(columns={'Text'})\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(test)\n",
    "test_df=test_df.transpose()\n",
    "test_df=test_df.reset_index()\n",
    "test_df=test_df.rename(columns={'index':'ID'})\n",
    "test_df=test_df.drop(columns={'Text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_tranpose=train_df.to_dict()\n",
    "test_audio_tranpose=test_df.to_dict()\n",
    "\n",
    "def save_json(dictionary, file_name):\n",
    "    json_data = json.dumps(dictionary, ensure_ascii=False)\n",
    "    with open(file_name, 'w', encoding='UTF8') as f:\n",
    "        f.write(json_data)\n",
    "        \n",
    "save_json(train_audio_tranpose,  data_dir + 'train_audio.json')\n",
    "save_json(test_audio_tranpose,  data_dir + 'test_audio.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### json 파일 다시 불러와서 확인하는 부분 ####\n",
    "\n",
    "train=[]\n",
    "test=[]\n",
    "\n",
    "with open(data_dir+'train_audio.json', 'r',encoding='UTF8') as file:\n",
    "    train = json.load(file)\n",
    "    \n",
    "with open(data_dir+'test_audio.json', 'r',encoding='UTF8') as file:\n",
    "    test = json.load(file)\n",
    "    \n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f92b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ca9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63370127",
   "metadata": {},
   "source": [
    "Extraction feature\n",
    "======================\n",
    "<span style='background-color:#fff5b1'>custpath 사용자에 맞게 변경 필요,</span>\n",
    "<span style='background-color:#fff5b1'>Wav 데이터 필요</span>\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc42216",
   "metadata": {},
   "source": [
    " -Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b988d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custpath = 'C:/Users/kungm/Desktop/ETRI/example/'#spectrogram 이미지 저장\n",
    "\n",
    "def feature_extractor(row):\n",
    "\n",
    "    name     = row[0]\n",
    "    y, sr = librosa.load(row[3], sr=22050)\n",
    "    try:\n",
    "        \n",
    "        audio = y\n",
    "    \n",
    "        #Spectrogram\n",
    "        plt.axis('off') # no axis(선선)\n",
    "        plt.axes([0., 0., 1., 1.])\n",
    "        melspec  = librosa.feature.melspectrogram(y=audio)\n",
    "        s_db     = librosa.power_to_db(melspec, ref=np.max) #소리 변환을 기록으로 인식할 수 있도록크기를 dB(데시벨)로 반전 크기를 전환합니다.\n",
    "        librosa.display.specshow(s_db)\n",
    "        savepath = os.path.join(custpath,name+'.png')\n",
    "        plt.savefig(savepath, bbox_inches=None, pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "    except:\n",
    "        print('File cannot open:',name)\n",
    "        return None\n",
    "    return savepath\n",
    "\n",
    "def resize_img(datafram):\n",
    "    for i in tqdm(range(len(datafram))):\n",
    "        file_path = datafram[i]\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.resize(img, (224, 224),interpolation = cv2.INTER_CUBIC)\n",
    "        cv2.imwrite(file_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb882a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgpaths_train = []\n",
    "imgpaths_test = []\n",
    "\n",
    "idx = 0\n",
    "\n",
    "\n",
    "for row in tqdm(train_df.values, leave=True):\n",
    "    Feature_Set  = feature_extractor(row)\n",
    "    imgpaths_train.append(Feature_Set)\n",
    "    \n",
    "\n",
    "for row in tqdm(test_df.values, leave=True):\n",
    "    Feature_Set  = feature_extractor(row)\n",
    "    imgpaths_test.append(Feature_Set)\n",
    "\n",
    "#ResNet50 input 형식에 맞게 변경\n",
    "resize_img(imgpaths_train)\n",
    "resize_img(imgpaths_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305bfb7",
   "metadata": {},
   "source": [
    "Data generation\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a460edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train={}\n",
    "data_train['ID']=[]\n",
    "data_train['path']=[]\n",
    "data_train['emotion']=[]\n",
    "\n",
    "data_train_feature= [imgpaths_train[i] for i in range(len(imgpaths_train))]\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    data_train['ID'].append(train_df['ID'][i])\n",
    "    data_train['path'].append(data_train_feature[i])\n",
    "    data_train['emotion'].append(train_df['Label'][i])\n",
    "    \n",
    "data_test={}\n",
    "data_test['ID']=[]\n",
    "data_test['path']=[]\n",
    "data_test['emotion']=[]\n",
    "\n",
    "data_test_feature= [imgpaths_test[i] for i in range(len(imgpaths_test))]\n",
    "\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    data_test['ID'].append(test_df['ID'][i])\n",
    "    data_test['path'].append(data_test_feature[i])\n",
    "    data_test['emotion'].append(test_df['Label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbaf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump( data_train, open( data_dir + 'data_train.pickle', 'wb' ) )\n",
    "pickle.dump( data_test, open( data_dir + 'data_test.pickle', 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### data 파일 다시 불러와서 확인하는 부분 ####\n",
    "\n",
    "data_train={}\n",
    "data_train['ID']=[]\n",
    "data_train['path']=[]\n",
    "data_train['emotion']=[]\n",
    "\n",
    "data_test={}\n",
    "data_test['ID']=[]\n",
    "data_test['path']=[]\n",
    "data_test['emotion']=[]\n",
    "\n",
    "with open( data_dir + 'data_train.pickle', 'rb') as f:\n",
    "    #파일을 열고 닫는 것을 자동으로 처리\n",
    "    data_train = pickle.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "with open( data_dir + 'data_test.pickle', 'rb') as f:\n",
    "    #파일을 열고 닫는 것을 자동으로 처리\n",
    "    data_test = pickle.load(f)\n",
    "\n",
    "        \n",
    "ds_train = pd.DataFrame.from_dict(data_train)\n",
    "ds_test = pd.DataFrame.from_dict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b08f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f9660",
   "metadata": {},
   "source": [
    "Spectrogram이 없는 사용자도 모델을 학습시킬 수 있는 data 생성\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train={}\n",
    "train['ID']=[]\n",
    "train['path']=[]\n",
    "train['emotion']=[]\n",
    "\n",
    "for i in tqdm(range(len(data_train['path']))):\n",
    "    seg_id= data_train['ID'][i]\n",
    "    train['ID'].append(seg_id)\n",
    "    img = cv2.imread(data_train['path'][i])\n",
    "    train['path'].append(img)\n",
    "    emotion = data_train['emotion'][i]\n",
    "    train['emotion'].append(emotion)\n",
    "    \n",
    "    \n",
    "test={}\n",
    "test['ID']=[]\n",
    "test['path']=[]\n",
    "test['emotion']=[]\n",
    "\n",
    "for i in tqdm(range(len(data_test['path']))):\n",
    "    seg_id= data_test['ID'][i]\n",
    "    test['ID'].append(seg_id)\n",
    "    img = cv2.imread(data_test['path'][i])\n",
    "    test['path'].append(img)\n",
    "    emotion = data_test['emotion'][i]\n",
    "    test['emotion'].append(emotion)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78cd0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump( train, open( data_dir + 'train_final.pickle', 'wb' ) )\n",
    "pickle.dump( test, open( data_dir + 'test_final.pickle', 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train={}\n",
    "train['ID']=[]\n",
    "train['path']=[]\n",
    "train['emotion']=[]\n",
    "    \n",
    "test={}\n",
    "test['ID']=[]\n",
    "test['path']=[]\n",
    "test['emotion']=[]\n",
    "\n",
    "with open( data_dir + 'train_final.pickle', 'rb') as f:\n",
    "    #파일을 열고 닫는 것을 자동으로 처리\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "with open( data_dir + 'test_final.pickle', 'rb') as f:\n",
    "    #파일을 열고 닫는 것을 자동으로 처리\n",
    "    test = pickle.load(f)\n",
    "\n",
    "        \n",
    "ds_train = pd.DataFrame.from_dict(train)\n",
    "ds_test = pd.DataFrame.from_dict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecaebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etri",
   "language": "python",
   "name": "etri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
